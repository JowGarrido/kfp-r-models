{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"ml-demos-garrido\"\n",
    "REGION = \"us-central1\"\n",
    "PIPELINE_ROOT = \"gs://garrido-dsml/pipelines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component)\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carregando o Iris Datasets \n",
    "iris = load_iris()\n",
    "\n",
    "# Construindo Dataframe\n",
    "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                 columns= iris['feature_names'] + ['target'])\n",
    "\n",
    "df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
    "columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species', 'targets']\n",
    "\n",
    "# Adicionando timestamp\n",
    "df.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë∑üèΩ‚Äç‚ôÄÔ∏è Realizando a ingest√£o do dataset para o BigQuery\n",
    "\n",
    "A partir do Iris dataset em mem√≥ria (i.e., o nosso `Pandas Dataframe`), podemos gerar um arquivo `Parquet` de forma bem simples para persistir os dados de forma bin√°ria no disco. A partir disso, ser√° poss√≠vel realizar um processo de ingest√£o em batch para o `BigQuery`, um processo *sem custo* que disponibilizar√° os nossos dados em uma estrutura serverless, de armazenamento colunar, com baixo custo e orientada √† recupera√ß√£o e processamento massivo de dados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Vamos come√ßar criando uma camada l√≥gica de dados no BigQuery, o nosso dataset\n",
    "{\n",
    "    bq --location=us-central1 mk -d iris\n",
    "} || { # catch\n",
    "    echo \"Dataset j√° existente\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery \n",
    "# Em seguida, vamos garantir que a nossa tabela ser√° recriada do zero\n",
    "DROP TABLE IF EXISTS iris.iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "# Vamos definir a nossa tabela, utilizando o statement DDL do BigQuery\n",
    "CREATE OR REPLACE TABLE iris.iris(\n",
    "    petal_length FLOAT64,\n",
    "    sepal_length FLOAT64,\n",
    "    petal_width FLOAT64,\n",
    "    sepal_width FLOAT64,\n",
    "    species FLOAT64,\n",
    "    targets STRING\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraindo os dados em Parquet para realizar ingest√£o\n",
    "df.to_parquet('features.parquet.gzip',\n",
    "              compression='gzip',\n",
    "             index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bq load \\\n",
    "    --source_format=PARQUET \\\n",
    "    iris.iris \\\n",
    "    features.parquet.gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT * FROM iris.iris LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë∑üèæ Desenvolvendo o pipeline de treinamento utilizando Kubeflow Pipelines\n",
    "\n",
    "Para orquestrar fluxos de trabalho de ML no Vertex Pipelines, primeiro precisamos descrever o fluxo de trabalho como um pipeline. Os pipelines de ML s√£o fluxos de trabalho de port√°teis e escal√°veis, baseados em cont√™ineres. Os pipelines de ML s√£o compostos por um conjunto de par√¢metros de entrada e uma lista de etapas. Cada tarefa √© uma inst√¢ncia de um componente do pipeline.\n",
    "\n",
    "Podemos usar os pipelines de ML para:\n",
    "\n",
    "- Aplicar estrat√©gias MLOps para automatizar e monitorar processos repet√≠veis\n",
    "- Testar diferentes conjuntos de hiperpar√¢metros, n√∫mero de etapas de treinamento ou itera√ß√µes, etc\n",
    "- Reutilizar etapas de pipeline para treinar uma nova vers√£o ou um novo modelo\n",
    "\n",
    "√â poss√≠vel usar o Vertex Pipelines para executar pipelines que foram criados usando o SDK do **Kubeflow Pipelines** ou o **TensorFlow Extended**.\n",
    "\n",
    "A seguir, utilizaremos o framework Kubeflow para criar um pipeline simples de treinamento de um classificador para o Iris Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['sklearn', 'google-cloud-bigquery[bqstorage,pandas]']\n",
    ")\n",
    "def get_data_from_bq(\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset]\n",
    "):\n",
    "    import os\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    project_number = os.environ['CLOUD_ML_PROJECT_ID']\n",
    "\n",
    "    bqclient = bigquery.Client(project=project_number)\n",
    "\n",
    "    # Download query results\n",
    "    query_string = \"\"\"\n",
    "    SELECT * FROM `ml-demos-garrido.iris.iris`\n",
    "    \"\"\"\n",
    "\n",
    "    df = (\n",
    "        bqclient.query(query_string)\n",
    "        .result()\n",
    "        .to_dataframe(\n",
    "            create_bqstorage_client=True,\n",
    "        )\n",
    "    )\n",
    "              \n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train.to_csv(dataset_train.path, index=False)\n",
    "    test.to_csv(dataset_test.path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['sklearn', 'pandas']\n",
    ")\n",
    "def train_sklearn_model(\n",
    "    dataset_train: Input[Dataset],\n",
    "    model: Output[Model]\n",
    "    \n",
    "):\n",
    "    import pandas as pd\n",
    "    \n",
    "    data = pd.read_csv(dataset_train.path)\n",
    "    \n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(data.drop(columns=['species', 'targets']), \n",
    "                      data.species)\n",
    "            \n",
    "    import pickle\n",
    "    \n",
    "    # Persistindo o modelo \n",
    "    filename = 'model.pkl'\n",
    "    pickle.dump(clf, open(model.path, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['sklearn', 'pandas', 'google-cloud-aiplatform']\n",
    ")\n",
    "def eval_model(\n",
    "    dataset_test: Input[Dataset],\n",
    "    sklearn_model: Input[Model],\n",
    "    endpoint_id: str,\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    smetrics: Output[Metrics]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    \n",
    "    data = pd.read_csv(dataset_test.path)\n",
    "    model = pickle.load(open(sklearn_model.path, 'rb'))\n",
    "    \n",
    "    score = model.score(\n",
    "        data.drop(columns=['species', 'targets']),\n",
    "        data.species\n",
    "        )\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    y_pred = model.predict(data.drop(columns=['species', 'targets']))\n",
    "    \n",
    "    metrics.log_confusion_matrix(\n",
    "       ['Setosa', 'Versicolour', 'Virginica'],\n",
    "       confusion_matrix(\n",
    "           data.species, y_pred\n",
    "       ).tolist(),  \n",
    "    )\n",
    "    \n",
    "    sklearn_model.metadata['test_score'] = float(score)\n",
    "    smetrics.log_metric('score', float(score))\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    import os\n",
    "    \n",
    "    # Definindo status de promo√ß√£o\n",
    "    promote = False\n",
    "    \n",
    "    # Inicializando Client da Vertex AI\n",
    "    project_number = os.environ['CLOUD_ML_PROJECT_ID']\n",
    "    aiplatform.init(project=project_number)\n",
    "    \n",
    "    # Instanciando Endpoint de produ√ß√£o\n",
    "    endpoint = aiplatform.Endpoint(endpoint_name=f'projects/ml-demos-garrido/locations/us-central1/endpoints/{endpoint_id}')\n",
    "    \n",
    "    # Recuperando id do modelo em produ√ß√£o\n",
    "    deployed_model_id = endpoint.list_models()[0].model\n",
    "    \n",
    "    # Instanciando Model de produ√ß√£o e recuperando uri do artefato\n",
    "    deployed_model = aiplatform.Model(model_name=deployed_model_id)\n",
    "    deployed_model_path = deployed_model.uri\n",
    "    \n",
    "    # Lendo o artefato do FUSE\n",
    "    fuse_path = deployed_model_path.replace('gs:/', 'gcs') + 'model.pkl'\n",
    "    deployed_model_loaded = pickle.load(open(fuse_path, 'rb'))\n",
    "    \n",
    "    baseline = deployed_model_loaded.score(\n",
    "        data.drop(columns=['species', 'targets']),\n",
    "        data.species\n",
    "        )\n",
    "  \n",
    "    print(f'O score baseline √©: {baseline}')\n",
    "    new_val = float(smetrics.metadata['score'])\n",
    "    print(f'O score novo √©: {new_val}')\n",
    "    \n",
    "    if new_val >= baseline:\n",
    "        promote = True\n",
    "        print('Modelo atinge requisitos potenciais de deployment') \n",
    "    else:\n",
    "        print('Modelo n√£o atinge requisitos de deployment')\n",
    "        \n",
    "    #return promote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1295: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name='iris-classifier-kfp-pipeline'\n",
    ")\n",
    "def pipeline(\n",
    "    endpoint_id: str = '7206946876350791680'\n",
    "):\n",
    "    ingest_data = get_data_from_bq()\n",
    "    train_op = train_sklearn_model(ingest_data.outputs['dataset_train'])\n",
    "    eval_op = eval_model(\n",
    "        dataset_test=ingest_data.outputs['dataset_test'],\n",
    "        sklearn_model=train_op.outputs['model'],\n",
    "        endpoint_id=endpoint_id\n",
    "    )\n",
    "        \n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='iris_classifier_pipeline.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë©üèæ‚Äçüíª Executando o pipeline de maneira serverless utilizando o Vertex Pipelines\n",
    "\n",
    "Ap√≥s desenvolvido o pipeline, podemos compilar a DAG utilizando o m√≥dulo `compile` da classe de compila√ß√£o do KFP.\n",
    "\n",
    "Com isso, iremos gerar um arquivo de configura√ß√£o, que descrever√° todas as dependencias da nossa esteira.\n",
    "\n",
    "Dessa forma, √© poss√≠vel gerar novas execu√ß√µes do mesmo pipeline atrav√©s da Vertex AI, simplesmente apontando um caminho para o arquivo de configura√ß√£o.\n",
    "\n",
    "Nesse caso, iremos utilizar o Cloud Storage (que √© um reposit√≥rio de leitura compat√≠vel com a API da Vertex AI) para armazenar o arquivo de configura√ß√£o, obtendo uma leitura r√°pida e f√°cil para execu√ß√µes posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://iris_classifier_pipeline.json [Content-Type=application/json]...\n",
      "/ [1 files][ 12.6 KiB/ 12.6 KiB]                                                \n",
      "Operation completed over 1 objects/12.6 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp iris_classifier_pipeline.json gs://garrido-dsml/pipelines/iris_classifier_pipeline.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/google/client/client.py:173: FutureWarning: AIPlatformClient will be deprecated in v2.0.0. Please use PipelineJob https://googleapis.dev/python/aiplatform/latest/_modules/google/cloud/aiplatform/pipeline_jobs.html in Vertex SDK. Install the SDK using \"pip install google-cloud-aiplatform\"\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/iris-classifier-kfp-pipeline-20220721032241?project=ml-demos-garrido\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "JOB_ID = 'iris-classifier-kfp-pipeline-{timestamp}'.format(timestamp=TIMESTAMP)\n",
    "COMPILED_PIPELINE_PATH = 'gs://garrido-dsml/pipelines/iris_classifier_pipeline.json'\n",
    "\n",
    "api_client = AIPlatformClient(project_id=PROJECT_ID,\n",
    "                           region=REGION)\n",
    "\n",
    "pipeline_run_name = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=COMPILED_PIPELINE_PATH,\n",
    "    job_id=JOB_ID,\n",
    "    enable_caching=False,\n",
    "    service_account='mlops-services@ml-demos-garrido.iam.gserviceaccount.com',\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
